%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Short Sectioned Assignment
% LaTeX Template
% Version 1.0 (5/5/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Frits Wenneker (http://www.howtotex.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[paper=a4, fontsize=11pt]{scrartcl} % A4 paper and 11pt font size
\usepackage[shortlabels]{enumitem}
\usepackage{float}
\usepackage{ctex}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{listings}
\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\usepackage{fourier} % Use the Adobe Utopia font for the document - comment this line to return to the LaTeX default
\usepackage[english]{babel} % English language/hyphenation
\usepackage{amsmath,amsfonts,amsthm} % Math packages

\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template

\usepackage{sectsty} % Allows customizing section commands
%\allsectionsfont{\centering \normalfont\scshape} % Make all sections centered, the default font and small caps
\usepackage{mathrsfs}
\usepackage{fancyhdr} % Custom headers and footers
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\algnewcommand\Break{\textbf{break}}

\usepackage{scrextend} % for addmargin
\usepackage{subcaption}
\graphicspath{{p3/}}
%\usepackage{algorithmic}
\usepackage[noend]{algpseudocode}
\usepackage{listings}
\pagestyle{fancyplain} % Makes all pages in the document conform to the custom headers and footers
\fancyhead{} % No page header - if you want one, create it in the same way as the footers below
\fancyfoot[L]{} % Empty left footer
\fancyfoot[C]{} % Empty center footer
\fancyfoot[R]{\thepage} % Page numbering for right footer
\renewcommand{\headrulewidth}{0pt} % Remove header underlines
\renewcommand{\footrulewidth}{0pt} % Remove footer underlines
\setlength{\headheight}{13.6pt} % Customize the height of the header

\numberwithin{equation}{section} % Number equations within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{figure}{section} % Number figures within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{table}{section} % Number tables within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)

\setlength\parindent{0pt} % Removes all indentation from paragraphs - comment this line for an assignment with lots of text

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} % Create horizontal rule command with 1 argument of height

\title{	
\normalfont \normalsize 
%\textsc{university, school or department name} \\ [25pt] % Your university, school and/or department name(s)
\horrule{0.5pt} \\[0.4cm] % Thin top horizontal rule
\huge Advanced Machine Learning - Homework 1\\ % The assignment title
\horrule{2pt} \\[0.5cm] % Thick bottom horizontal rule
}

\author{宁雪妃} % Your name
%\author{Xuefei Ning} % Your name

\date{\normalsize\today} % Today's date or a custom date

\begin{document}

\maketitle % Print the title

%----------------------------------------------------------------------------------------
%	PROBLEM 1
%----------------------------------------------------------------------------------------
\section{Problem 1}
\begin{enumerate}[a]
\item \textbf{Consider a real, symmetric matrix $\Sigma$ whose eigenvalue equation is given by:}
  \[
  \Sigma u_i = \lambda_i u_i
  \]
  \textbf{By taking the complex conjugate of this equation and subtracting the original equation, and then forming the inner product with eigenvector $u_i$, show that the eigenvalues $\lambda_i$ are real. Similarly, use the symmetry property of $\Sigma$ to show that two eigenvectors $u_i$ and $u_j$ will be orthogonal provided $\lambda_j \neq \lambda_i$. Finally, show that without loss of generality, the set of eigenvectors can be chosen to be orthonormal, even if some of the eigenvalues are zero.}

  证明所有特征值$\lambda_i$均为实数, 对特征方程同时左乘$u_i$的共轭转置$u_i^{'}$
  \[
  u_i^{'} \Sigma u_i = \lambda_i u_i^{'} u_i
  \]
  又由于$\Sigma$为对称实矩阵, 有其共轭转置矩阵与原矩阵相同: $\Sigma = \Sigma^{'}$, 有$u_i^{'} \Sigma u_i = (\Sigma u_i)' u_i = \lambda_i^{*} u_i' u_i$。与上式对比, 由于$u_i \neq 0 \leadsto u_i' u_i > 0$, 所以$\lambda_i = \lambda_i^{*}$, 即$\lambda_i$为实数。\\

  证明实对称矩阵对应不同特征值的特征向量正交: 设$u_i$和$u_j$分别为对应$\Sigma$的不同特征值$\lambda_i \neq \lambda_j$的特征向量. 有:
  \begin{equation}
  \begin{split}
    \Sigma u_i & = \lambda_i u_i \\
    \Sigma u_j & = \lambda_j u_j
  \end{split}
  \end{equation}
  有:
  \begin{equation}
  \begin{split}
    u_j^T \lambda_i u_i = u_j^T \Sigma u_i = {(\Sigma u_j)}^T u_i & = \lambda_j u_j^T u_i \\
    \leadsto (\lambda_i - \lambda_j) u_j^T u_i & = 0
  \end{split}
  \end{equation}
  由于$\lambda_i \neq \lambda_j$, 所以一定有$u_j^T u_i = 0$, 即对应不同特征值的特征向量正交。\\

  根据schur定理可知: 任何特征值均为实数的矩阵一定正交相似于上三角阵, 又由于$\Sigma$对称, 所以该上三角阵为对角阵, $\Sigma$一定正交相似于对角阵。相似矩阵中即为特征向量的集合。
  也可以从说明对应同个特征值 \(可能为0\) 的特征向量可以取为正交(已经证明对应不同特征值的特征向量相互正交)来说明: 由于实对称矩阵有性质: r重特征值对应的特征空间也有r重. 所以一定可以在特征空间里做正交化得到对应这个r重特征值的r个正交的特征向量。这样所有的特征向量集合就可以取成两两正交的集合了。

\item \textbf{Refer to slides about PCA, where we perform eigen-decomposition on}
  \[
  A = \frac{1}{N} \displaystyle \sum_{1=1}^{N} x_i x_i^T
  \]
  \textbf{Prove $A$ is a symmetric and positive semi-definite matrix.}

  A对称:
  \[
  \begin{split}
    A^T = \frac{1}{N} \displaystyle\sum_{i=1}^{N} x_i x_i^T = A
  \end{split}
  \]
  A半正定:
  \[
  \begin{split}
    \forall y, y^T A y = \frac{1}{N} \displaystyle\sum_{i=1}^{N} (x_i^T y)^T (x_i^T y) \geq 0
  \end{split}
  \]
\end{enumerate}

% ----
\section{Problem 2}
\textbf{Given a set of i.i.d data $X = \{x_1, \dots, x_N\}$ drawn from $N(x; \mu, \Sigma)$, we want to estimate $(\mu, \Sigma)$ by MLE.}
\begin{enumerate}[a]
\item \textbf{Write the log likelyhood function.}
  \[
  \begin{split}
    L(X) = P(\{x_1, \dots, x_N\} | \mu, \Sigma) & = \prod_{i=1}^{N} P(x_i | \mu, \Sigma) \\
    & = \prod_{i=1}^{N} \frac{1}{\sqrt{{(2\pi)}^n|\Sigma|}} \exp(-\frac{1}{2} {(x_i - \mu)}^T \Sigma^{-1} (x_i - \mu))\\
    & = \frac{1}{{(2\pi)}^{\frac{nN}{2}} {\Sigma}^{\frac{N}{2}}} \exp(-\frac{1}{2} \sum_{i=1}^N {(x_i - \mu)}^T \Sigma^{-1} (x_i - \mu)) \\
    \mbox{logL}(X) = \log(L(X)) & = - \frac{N}{2} \log(|\Sigma|) - \frac{Nn}{2} \log(2\pi) - \frac{1}{2} \sum_{i=1}^N {(x_i - \mu)}^T \Sigma^{-1} (x_i - \mu)
  \end{split}
  \]
\item \textbf{Take the derivative of log likelyhood function w.r.t $\mu$, show that}
  \[
  \mu_{ML} = \frac{1}{N} \displaystyle \sum_{i=1}^{N} x_i
  \]

  \[
  \frac{\partial \mbox{logL(X)}}{\partial \mu} = \sum_{i=1}^N \Sigma^{-1} (x_i - \mu) = \Sigma^{-1} \sum_{i=1}^N (x_i - \mu) = 0  \]
  由于$\Sigma^{-1}$可逆, 所以要该偏导为0, 必须:
  \[
  \sum_{i=1}^N (x_i - \mu) = 0 \leadsto \mu_{ML} = \frac{1}{N} \sum_{i=1}^N x_i
  \]

\item \textbf{Take the derivative of log likelihood function w.r.t. $\Sigma$, show that}
  \[
  \sum _{ML} = \frac{1}{N} \displaystyle\sum_{i=1}{N} (x_i - \mu_{ML})(x_i - \mu_{ML})^T
  \]
  \[
  \frac{\partial \mbox{logL(X)}}{\partial \Sigma} = -\frac{N}{2} \Sigma^{-T} + \frac{1}{2} \sum_{i=1}^N \Sigma^{-T} (x_i - \mu)(x_i - \mu)^T \Sigma^{-T} = \frac{\Sigma^{-T}}{2} (\sum_{i=1}^N (x_i - \mu){(x_i - \mu)}^T \Sigma^{-T} - N) = 0
  \]
  由于$\Sigma^{-1}$可逆, 有$\frac{1}{N} \sum_{i=1}^N (x_i - \mu){(x_i - \mu)}^T \Sigma^{-T} = 1$, 两边同时右乘$\Sigma$, 注意有$\Sigma^T = \Sigma$, 得到:
  \[
  \Sigma_{ML} = \frac{1}{N} \sum_{i=1}^N (x_i - \mu_{ML}){(x_i - \mu_{ML})}^T
  \]

\item \textbf{Evaluate expectations of $\mu_{ML}$ and $\Sigma_{ML}$, show $\mu_{ML}$ is unbiased but $\Sigma_{ML}$ is biased.}
  \[
  E(\mu_{ML}) = \frac{1}{N} \sum_{i=1}^N E(x_i) = \frac{1}{N} \sum_{i=1}^N \mu = \mu
  \]
  $\mu_{ML}$为无偏估计.
  \[
  \begin{split}
    E(\Sigma_{ML}) & = \frac{1}{N} \sum_{i=1}^N E(x_i x_i^T + {\mu_{ML}}^2 - \mu_{ML} x_i^T - x_i \mu_{ML}^T) = \Sigma + E(\mu_{ML} \mu_{ML}^T) - \frac{1}{N} E(\sum_{i=1}^N \mu_{ML} x_i^T) - \frac{1}{N} E(\sum_{i=1}^N x_i \mu_{ML}^T)\\
    & = \Sigma - E(\mu_{ML} \mu_{ML}^T) \neq \Sigma
  \end{split}
  \]
  $\Sigma_{ML}$为有偏估计.
\end{enumerate}

% ----
\section{Problem 3}
\textbf{For support vector machines, the class-conditional distributions may overlap, we therefore modify the support vector machine so as to allow some of the training points to be misclassified. For un-separable case, the formalization of the optimal problem becomes: Given $\{x_i, y_i\}, i=1, \dots, N, y_i \in \{−1, 1\}$ are training examples,}
\[
\begin{split}
  \displaystyle\min_{\omega, b} \frac{{\|w\|}^2}{2} & + C \displaystyle \sum_{i=1}^{N} \xi_i\\
  \mbox{s.t. } y^{(i)}(\omega^T x^{(i)} & + b) \geq 1 - \xi_i, 1 \leq i \leq N \\
  \xi_i \geq 0, & 1 \leq i \leq N
  \end{split}
\]
\textbf{where the $\xi_i$ denotes the slack variable penalty, and the parameter $C$ controls the trade-off betwwen the slack variable penalty and the margin. Please give the solutions of $\omega$ and $b$.}
\begin{enumerate}[a]
\item \textbf{Give the corresponding Lagrangian and the set of KKT conditions.}
  Lagrangian:
  \[
  l(\omega, b, \{\xi_i\}, \{a_i\}, \{\beta_i\}) = \frac{{\|\omega\|}^2}{2} + C \sum_{i=1}^N \xi_i - \sum_{i=1}^N a_i(y^{(i)}(w^T x^{(i)} + b) + \xi_i - 1) - \sum_{i=1}^N \beta_i \xi_i
  \]
  对应的KKT条件为:
  \[
  \begin{split}
    a_i(y^{(i)}(w^T x^{(i)} + b) + \xi_i - 1) = 0 \\
    y^{(i)}(w^T x^{(i)} + b) + \xi_i - 1 \geq 0\\
    \xi_i \geq 0\\
    a_i \geq 0\\
    \beta_i \geq 0\\
    \mbox{for } i = 1, \dots, N
  \end{split}
  \]
\item \textbf{Optimize out $\omega$, $b$ and $\{\xi_i\}$.}
  原目标函数转换为Lagrangian后, 在可行解空间与
  \[
  \max_{\{a_i\}, \{\beta_i\}}(l(\omega, b, \{\xi_i\}, \{a_i\}, \{\beta_i\}))
  \]
  的值相等, 由于原问题满足正则条件, 所以dual问题$\max_{\{a_i\}, \{\beta_i\}} \min_{\omega, b, \{\xi_i\}} l(\omega, b, \{\xi_i\}, \{a_i\}, \{\beta_i\})$的最优解与原问题的最优解相等(即duality gap为0)。只需要解出dual问题的最优解, 即可根据dual问题内部min问题的最优解$\omega, b = f^{*}(\{a_i\}, \{\beta_i\})$得到$\omega$和$b$。现在先解dual问题内部min问题的最优解:
  \[
  \begin{split}
    \frac{\partial l(\omega, b, \{\xi_i\}, \{a_i\}, \{\beta_i\})}{\partial \omega} = \omega - \sum_{i=1}^N a_i y^{(i)} x^{(i)} = 0\\
    \frac{\partial l(\omega, b, \{\xi_i\}, \{a_i\}, \{\beta_i\})}{\partial b} = -\sum_{i=1}^N a_i y^{(i)} = 0\\
    \frac{\partial l(\omega, b, \{\xi_i\}, \{a_i\}, \{\beta_i\})}{\partial \xi_i} = C - a_i - \beta_i = 0\\
  \end{split}
  \]
  将$\omega = \sum_{i=1}^N a_i y^{(i)} x^{(i)}$, $\sum_{i=1}^N a_i y^{(i)} = 0$和$C - a_i - \beta_i = 0$代入$l$的表达式得到:
  \[
  \begin{split}
    l(\{a_i\}, \{\beta_i\}) & = \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N a_i a_j y^{(i)} y^{(j)} x^{(i)} x^{(j)} + C \sum_{i=1}^N \xi_i + \sum_{i=1}^N a_i - \sum_{i=1}^N\sum_{j=1}^N a_i a_j y^{(i)} y^{(j)} x^{(i)} x^{(j)} - \sum_{i=1}^N a_i \xi_i - \sum_{i=1}^N \beta_i \xi_i \\
    & = -\frac{1}{2} \sum_{i=1}^N\sum_{j=1}^N a_i a_j y^{(i)} y^{(j)} x^{(i)} x^{(j)} + \sum_{i=1}^N a_i
  \end{split}
  \]
  注意这个优化式里没有$\beta_i$, 但是$\beta_i \geq 0$的限制和$C - a_i - \beta_i = 0$给$a_i$加入了一条额外的限制$a_i \leq C$。

\item \textbf{Give the dual Lagrangian.}
  dual Lagrangian为:
  \[
  l(\{a_i\}, \{\beta_i\}) = \min_{\omega, b, \{\xi_i\}} l(\omega, b, \{\xi_i\}, \{a_i\}, \{\beta_i\}) = -\frac{1}{2} \sum_{i=1}^N\sum_{j=1}^N a_i a_j y^{(i)} y^{(j)} x^{(i)} x^{(j)} + \sum_{i=1}^N a_i\\
  \]
  对上式在约束条件下最大化, 约束条件为:
  \[
  \begin{split}
    0 \leq a_i \leq C, 1 \leq i \leq N\\
    \sum_{i=1}^{N} a_i y_i = 0
  \end{split}
  \]
\item \textbf{Give the final solution for $\omega$ and the numerically stable solution of $b$.}
  使用solver解出上面对于$\{a_i\}$的二次规划问题(QP), 找到了最优的$a_i$, 构造对应的最优的$\omega^{*} = \sum_{i=1}^N a_i y^{(i)} x^{(i)}$.
  计算$b$, 只需要找到一个$a_j > 0$且$a_j \neq C$(即不是一个有分类软错误的点)的数据点$x_j$, 解$y^{(j)}(\omega^{*} x^{(j)} + b) - 1 = 0 \leadsto b = y^{(j)} - \omega^{*}x^{(j)}$。
\end{enumerate}
\end{document}
